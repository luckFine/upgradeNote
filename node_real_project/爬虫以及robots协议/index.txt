爬虫是一种自动获取网页内容的程序。
是搜索引擎的重要组成部分，爬取之后放到数据库里，做各种索引。

robots.txt是一个文本文件。
是一个协议，不是命令。
这个文件是爬虫要查看的第一个文件。
这个文件会告诉爬虫在服务器上什么文件可以被查看。
搜索机器人会按照文件中的内容来确定访问范围。
对于不允许访问的范围，受法律保护。

我们能够爬到html，但是ajax不到，因为html拉回来了，ajax却还没回来。
总之，爬虫拿到的是html代码。


node爬虫依赖的模块：
express，request，cheerio[操作从服务端拉取的数据]